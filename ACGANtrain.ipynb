{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./asset/data/mnist/train-images-idx3-ubyte.gz\n",
      "Extracting ./asset/data/mnist/train-labels-idx1-ubyte.gz\n",
      "Extracting ./asset/data/mnist/t10k-images-idx3-ubyte.gz\n",
      "Extracting ./asset/data/mnist/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'train_1:1' shape=(32,) dtype=int32>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import sugartensor as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "__author__ = 'buriburisuri@gmail.com'\n",
    "# only use gpu 1\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "# set log level to debug\n",
    "tf.sg_verbosity(10)\n",
    "\n",
    "#\n",
    "# hyper parameters\n",
    "#\n",
    "\n",
    "batch_size = 32   # batch size\n",
    "cat_dim = 10   # total categorical factor\n",
    "con_dim = 2    # total continuous factor\n",
    "rand_dim = 38  # total random latent dimension\n",
    "\n",
    "\n",
    "#\n",
    "# create generator & discriminator function\n",
    "#\n",
    "\n",
    "# generator network\n",
    "def generator(tensor):\n",
    "\n",
    "    # reuse flag\n",
    "    reuse = len([t for t in tf.global_variables() if t.name.startswith('generator')]) > 0\n",
    "\n",
    "    with tf.sg_context(name='generator', size=4, stride=2, act='relu', bn=True, reuse=reuse):\n",
    "        res = (tensor\n",
    "               .sg_dense(dim=1024, name='fc1')\n",
    "               .sg_dense(dim=7*7*128, name='fc2')\n",
    "               .sg_reshape(shape=(-1, 7, 7, 128))\n",
    "               .sg_upconv(dim=64, name='conv1')\n",
    "               .sg_upconv(dim=1, act='sigmoid', bn=False, name='conv2'))\n",
    "    return res\n",
    "\n",
    "\n",
    "def discriminator(tensor):\n",
    "\n",
    "    # reuse flag\n",
    "    reuse = len([t for t in tf.global_variables() if t.name.startswith('discriminator')]) > 0\n",
    "\n",
    "    with tf.sg_context(name='discriminator', size=4, stride=2, act='leaky_relu', reuse=reuse):\n",
    "        # shared part\n",
    "        shared = (tensor\n",
    "                  .sg_conv(dim=64, name='conv1')\n",
    "                  .sg_conv(dim=128, name='conv2')\n",
    "                  .sg_flatten()\n",
    "                  .sg_dense(dim=1024, name='fc1'))\n",
    "\n",
    "        # discriminator end\n",
    "        disc = shared.sg_dense(dim=1, act='linear', name='disc').sg_squeeze()\n",
    "\n",
    "        # shared recognizer part\n",
    "        recog_shared = shared.sg_dense(dim=128, name='recog')\n",
    "\n",
    "        # categorical auxiliary classifier end\n",
    "        cat = recog_shared.sg_dense(dim=cat_dim, act='linear', name='cat')\n",
    "\n",
    "        # continuous auxiliary classifier end\n",
    "        con = recog_shared.sg_dense(dim=con_dim, act='sigmoid', name='con')\n",
    "\n",
    "        return disc, cat, con\n",
    "\n",
    "\n",
    "#\n",
    "# inputs\n",
    "#\n",
    "\n",
    "# MNIST input tensor ( with QueueRunner )\n",
    "data = tf.sg_data.Mnist(batch_size=batch_size)\n",
    "\n",
    "# input images and label\n",
    "x = data.train.image\n",
    "y = data.train.label\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from asset/train/model.ckpt-106516\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-14045023ff70>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;31m# do training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m \u001b[0malt_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_interval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_ep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mep_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mearly_stop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/sugartensor/sg_train.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(**kwargs)\u001b[0m\n\u001b[1;32m    311\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m         \u001b[0;31m# create session\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 313\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0msv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmanaged_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConfigProto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mallow_soft_placement\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    314\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m             \u001b[0;31m# console logging loop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/contextlib.py\u001b[0m in \u001b[0;36m__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"generator didn't yield\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/supervisor.py\u001b[0m in \u001b[0;36mmanaged_session\u001b[0;34m(self, master, config, start_standard_services, close_summary_writer)\u001b[0m\n\u001b[1;32m    947\u001b[0m       sess = self.prepare_or_wait_for_session(\n\u001b[1;32m    948\u001b[0m           \u001b[0mmaster\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmaster\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 949\u001b[0;31m           start_standard_services=start_standard_services)\n\u001b[0m\u001b[1;32m    950\u001b[0m       \u001b[0;32myield\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    951\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/supervisor.py\u001b[0m in \u001b[0;36mprepare_or_wait_for_session\u001b[0;34m(self, master, config, wait_for_checkpoint, max_wait_secs, start_standard_services)\u001b[0m\n\u001b[1;32m    705\u001b[0m           \u001b[0mmax_wait_secs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_wait_secs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    706\u001b[0m           init_feed_dict=self._init_feed_dict, init_fn=self._init_fn)\n\u001b[0;32m--> 707\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_write_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    708\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mstart_standard_services\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    709\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_standard_services\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/supervisor.py\u001b[0m in \u001b[0;36m_write_graph\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    608\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_logdir\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    609\u001b[0m       training_util.write_graph(self._graph.as_graph_def(add_shapes=True),\n\u001b[0;32m--> 610\u001b[0;31m                                 self._logdir, \"graph.pbtxt\")\n\u001b[0m\u001b[1;32m    611\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_summary_writer\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph_added_to_summary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    612\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_summary_writer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/graph_io.py\u001b[0m in \u001b[0;36mwrite_graph\u001b[0;34m(graph_or_graph_def, logdir, name, as_text)\u001b[0m\n\u001b[1;32m     65\u001b[0m   \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mas_text\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m     \u001b[0mfile_io\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0matomic_write_string_to_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph_def\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0mfile_io\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0matomic_write_string_to_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgraph_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSerializeToString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# labels for discriminator\n",
    "y_real = tf.ones(batch_size)\n",
    "y_fake = tf.zeros(batch_size)\n",
    "\n",
    "# discriminator labels ( half 1s, half 0s )\n",
    "y_disc = tf.concat([y, y * 0], 0)\n",
    "\n",
    "# categorical latent variable\n",
    "z_cat = tf.multinomial(tf.ones((batch_size, cat_dim), dtype=tf.sg_floatx) / cat_dim, 1).sg_squeeze().sg_int()\n",
    "# continuous latent variable\n",
    "z_con = tf.random_normal((batch_size, con_dim))\n",
    "# random latent variable dimension\n",
    "z_rand = tf.random_normal((batch_size, rand_dim))\n",
    "# latent variable\n",
    "z = tf.concat([z_cat.sg_one_hot(depth=cat_dim), z_con, z_rand], 1)\n",
    "\n",
    "\n",
    "#\n",
    "# Computational graph\n",
    "#\n",
    "\n",
    "# generator\n",
    "gen = generator(z)\n",
    "\n",
    "# add image summary\n",
    "tf.sg_summary_image(x, name='real')\n",
    "tf.sg_summary_image(gen, name='fake')\n",
    "\n",
    "# discriminator\n",
    "disc_real, cat_real, _ = discriminator(x)\n",
    "disc_fake, cat_fake, con_fake = discriminator(gen)\n",
    "\n",
    "\n",
    "#\n",
    "# loss\n",
    "#\n",
    "\n",
    "# discriminator loss\n",
    "loss_d_r = disc_real.sg_bce(target=y_real, name='disc_real')\n",
    "loss_d_f = disc_fake.sg_bce(target=y_fake, name='disc_fake')\n",
    "loss_d = (loss_d_r + loss_d_f) / 2\n",
    "\n",
    "\n",
    "# generator loss\n",
    "loss_g = disc_fake.sg_bce(target=y_real, name='gen')\n",
    "\n",
    "# categorical factor loss\n",
    "loss_c_r = cat_real.sg_ce(target=y, name='cat_real')\n",
    "loss_c_d = cat_fake.sg_ce(target=z_cat, name='cat_fake')\n",
    "loss_c = (loss_c_r + loss_c_d) / 2\n",
    "\n",
    "# continuous factor loss\n",
    "loss_con = con_fake.sg_mse(target=z_con, name='con').sg_mean(dims=1)\n",
    "\n",
    "\n",
    "#\n",
    "# train ops\n",
    "#\n",
    "\n",
    "# discriminator train ops\n",
    "train_disc = tf.sg_optim(loss_d + loss_c + loss_con, lr=0.0001, category='discriminator')\n",
    "# generator train ops\n",
    "train_gen = tf.sg_optim(loss_g + loss_c + loss_con, lr=0.001, category='generator')\n",
    "\n",
    "\n",
    "#\n",
    "# training\n",
    "#\n",
    "\n",
    "# def alternate training func\n",
    "@tf.sg_train_func\n",
    "def alt_train(sess, opt):\n",
    "    l_disc = sess.run([loss_d, train_disc])[0]  # training discriminator\n",
    "    l_gen = sess.run([loss_g, train_gen])[0]  # training generator\n",
    "    return np.mean(l_disc) + np.mean(l_gen)\n",
    "\n",
    "# do training\n",
    "alt_train(log_interval=10, max_ep=30, ep_size=data.train.num_batch, early_stop=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
